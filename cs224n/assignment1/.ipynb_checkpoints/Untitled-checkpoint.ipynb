{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from q1_softmax import softmax\n",
    "from q2_gradcheck import gradcheck_naive\n",
    "from q2_sigmoid import sigmoid, sigmoid_grad\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function\n",
    "\n",
    "    Implement a function that normalizes each row of a matrix to have\n",
    "    unit length.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    a = np.sqrt(np.sum(x**2, axis=1)).reshape(((len(x), 1)))\n",
    "    x = x / a\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def test_normalize_rows():\n",
    "    print(\"Testing normalizeRows...\")\n",
    "    x = normalizeRows(np.array([[3.0,4.0],[1, 2]]))\n",
    "    print(x)\n",
    "    ans = np.array([[0.6,0.8],[0.4472136,0.89442719]])\n",
    "    assert np.allclose(x, ans, rtol=1e-05, atol=1e-06)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxCostAndGradient(predicted, target, outputVectors, dataset):\n",
    "    \"\"\" Softmax cost function for word2vec models\n",
    "\n",
    "    Implement the cost and gradients for one predicted word vector\n",
    "    and one target word vector as a building block for word2vec\n",
    "    models, assuming the softmax prediction function and cross\n",
    "    entropy loss.\n",
    "\n",
    "    Arguments:\n",
    "    predicted -- numpy ndarray, predicted word vector (\\hat{v} in\n",
    "                 the written component)\n",
    "    target -- integer, the index of the target word\n",
    "    outputVectors -- \"output\" vectors (as rows) for all tokens\n",
    "    dataset -- needed for negative sampling, unused here.\n",
    "\n",
    "    Return:\n",
    "    cost -- cross entropy cost for the softmax word prediction\n",
    "    gradPred -- the gradient with respect to the predicted word\n",
    "           vector\n",
    "    grad -- the gradient with respect to all the other word\n",
    "           vectors\n",
    "\n",
    "    We will not provide starter code for this function, but feel\n",
    "    free to reference the code you previously wrote for this\n",
    "    assignment!\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    ipdb.set_trace()\n",
    "    gradPred = np.zeros(outputVectors.shape)\n",
    "    grad = np.zeros(outputVectors.shape)\n",
    "    new_vector = np.concatenate((predicted.reshape((1, len(predicted))), outputVectors[:target], outputVectors[target+1:]), axis=0)\n",
    "    y_hat = softmax(new_vector.T)[0]\n",
    "    cost = -1 * np.log(y_hat[target])\n",
    "    gradPred[target] = -1 * new_vector[target]\n",
    "    for i in np.arange(outputVectors.shape[0]):\n",
    "        if i != target:\n",
    "            gradPred[target] += y_hat[i] * new_vector[i]\n",
    "    grad\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradPred, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for skip-gram ====\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-502cf96322c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n\u001b[1;32m     17\u001b[0m     skipgram, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n\u001b[0;32m---> 18\u001b[0;31m     dummy_vectors)\n\u001b[0m\u001b[1;32m     19\u001b[0m gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n\u001b[1;32m     20\u001b[0m     skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient),\n",
      "\u001b[0;32m~/course/nlp/cs224n/assignment1/q2_gradcheck.py\u001b[0m in \u001b[0;36mgradcheck_naive\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mrndstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrndstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mfx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Evaluate function value at original point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m        \u001b[0;31m# Do not change this!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-502cf96322c2>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(vec)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==== Gradient check for skip-gram ====\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n\u001b[0;32m---> 17\u001b[0;31m     skipgram, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n\u001b[0m\u001b[1;32m     18\u001b[0m     dummy_vectors)\n\u001b[1;32m     19\u001b[0m gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
      "\u001b[0;32m<ipython-input-14-b4888273aade>\u001b[0m in \u001b[0;36mword2vec_sgd_wrapper\u001b[0;34m(word2vecModel, tokens, wordVectors, dataset, C, word2vecCostAndGradient)\u001b[0m\n\u001b[1;32m    122\u001b[0m         c, gin, gout = word2vecModel(\n\u001b[1;32m    123\u001b[0m             \u001b[0mcenterword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputVectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             dataset, word2vecCostAndGradient)\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatchsize\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgin\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatchsize\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-b4888273aade>\u001b[0m in \u001b[0;36mskipgram\u001b[0;34m(currentWord, C, contextWords, tokens, inputVectors, outputVectors, dataset, word2vecCostAndGradient)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m### YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;31m### END YOUR CODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = type('dummy', (), {})()\n",
    "def dummySampleTokenIdx():\n",
    "    return random.randint(0, 4)\n",
    "\n",
    "def getRandomContext(C):\n",
    "    tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    return tokens[random.randint(0,4)], \\\n",
    "        [tokens[random.randint(0,4)] for i in range(2*C)]\n",
    "dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "dataset.getRandomContext = getRandomContext\n",
    "\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "print(\"==== Gradient check for skip-gram ====\")\n",
    "gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "    skipgram, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n",
    "    dummy_vectors)\n",
    "gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "    skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient),\n",
    "    dummy_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for skip-gram ====\n",
      "> \u001b[0;32m<ipython-input-22-c4b8c4f029e0>\u001b[0m(30)\u001b[0;36msoftmaxCostAndGradient\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     29 \u001b[0;31m    \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 30 \u001b[0;31m    \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     31 \u001b[0;31m    \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> new_vector = np.concatenate((predicted.reshape((1, len(predicted))), outputVectors[1:]), axis=0)     x = outputVectors.dot(predicted)\n",
      "*** SyntaxError: invalid syntax\n",
      "ipdb> new_vector = np.concatenate((predicted.reshape((1, len(predicted))), outputVectors[1:]), axis=0)\n",
      "ipdb> softmax(new_vector)\n",
      "array([[0.25351131, 0.4215227 , 0.32496599],\n",
      "       [0.25351131, 0.4215227 , 0.32496599],\n",
      "       [0.25351131, 0.4215227 , 0.32496599],\n",
      "       [0.25351131, 0.4215227 , 0.32496599],\n",
      "       [0.25351131, 0.4215227 , 0.32496599]])\n",
      "ipdb>  softmax(new_vector.T)\n",
      "array([[0.1958397 , 0.28703377, 0.20460871, 0.13153155, 0.18098626],\n",
      "       [0.1958397 , 0.28703377, 0.20460871, 0.13153155, 0.18098626],\n",
      "       [0.1958397 , 0.28703377, 0.20460871, 0.13153155, 0.18098626]])\n",
      "ipdb> np.concatenate((predicted.reshape((1, len(predicted))), outputVectors[:target], outputVectors[target+1:]), axis=0)\n",
      "array([[-0.96735714, -0.02182641,  0.25247529],\n",
      "       [ 0.18289107,  0.76098587, -0.62245591],\n",
      "       [-0.61517874,  0.5147624 , -0.59713884],\n",
      "       [-0.33867074, -0.80966534, -0.47931635],\n",
      "       [-0.52629529, -0.78190408,  0.33412466]])\n",
      "ipdb> softmax(new_vector)[0]\n",
      "array([0.25351131, 0.4215227 , 0.32496599])\n",
      "ipdb> softmax(new_vector.T)\n",
      "array([[0.1958397 , 0.28703377, 0.20460871, 0.13153155, 0.18098626],\n",
      "       [0.1958397 , 0.28703377, 0.20460871, 0.13153155, 0.18098626],\n",
      "       [0.1958397 , 0.28703377, 0.20460871, 0.13153155, 0.18098626]])\n",
      "ipdb> softmax(new_vector.T)[0]\n",
      "array([0.1958397 , 0.28703377, 0.20460871, 0.13153155, 0.18098626])\n",
      "ipdb> softmax(new_vector.T)[0][labels]\n",
      "*** NameError: name 'labels' is not defined\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def getNegativeSamples(target, dataset, K):\n",
    "    \"\"\" Samples K indexes which are not the target \"\"\"\n",
    "\n",
    "    indices = [None] * K\n",
    "    for k in range(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == target:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        indices[k] = newidx\n",
    "    return indices\n",
    "\n",
    "\n",
    "def negSamplingCostAndGradient(predicted, target, outputVectors, dataset,\n",
    "                               K=10):\n",
    "    \"\"\" Negative sampling cost function for word2vec models\n",
    "\n",
    "    Implement the cost and gradients for one predicted word vector\n",
    "    and one target word vector as a building block for word2vec\n",
    "    models, using the negative sampling technique. K is the sample\n",
    "    size.\n",
    "\n",
    "    Note: See test_word2vec below for dataset's initialization.\n",
    "\n",
    "    Arguments/Return Specifications: same as softmaxCostAndGradient\n",
    "    \"\"\"\n",
    "\n",
    "    # Sampling of indices is done for you. Do not modify this if you\n",
    "    # wish to match the autograder and receive points!\n",
    "    indices = [target]\n",
    "    indices.extend(getNegativeSamples(target, dataset, K))\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradPred, grad\n",
    "\n",
    "#def softmaxCostAndGradient(predicted, target, outputVectors, dataset):\n",
    "def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors,\n",
    "             dataset, word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec\n",
    "\n",
    "    Implement the skip-gram model in this function.\n",
    "\n",
    "    Arguments:\n",
    "    currentWord -- a string of the current center word\n",
    "    C -- integer, context size\n",
    "    contextWords -- list of no more than 2*C strings, the context words\n",
    "    tokens -- a dictionary that maps words to their indices in\n",
    "              the word vector list\n",
    "    inputVectors -- \"input\" word vectors (as rows) for all tokens\n",
    "    outputVectors -- \"output\" word vectors (as rows) for all tokens\n",
    "    word2vecCostAndGradient -- the cost and gradient function for\n",
    "                               a prediction vector given the target\n",
    "                               word vectors, could be one of the two\n",
    "                               cost functions you implemented above.\n",
    "\n",
    "    Return:\n",
    "    cost -- the cost function value for the skip-gram model\n",
    "    grad -- the gradient with respect to the word vectors\n",
    "    \"\"\"\n",
    "\n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "    #ipdb.set_trace()\n",
    "    ### YOUR CODE HERE\n",
    "    for i in np.arange(len(tokens)):\n",
    "        temp_cost, gradPred, grad = softmaxCostAndGradient(inputVectors[i], i, outputVectors, dataset)\n",
    "        cost += temp_cost\n",
    "        gradIn += gradPred\n",
    "        gradOut += grad\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradIn, gradOut\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_word2vec():\n",
    "    \"\"\" Interface to the dataset for negative sampling \"\"\"\n",
    "    dataset = type('dummy', (), {})()\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0,4)], \\\n",
    "            [tokens[random.randint(0,4)] for i in range(2*C)]\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "    print(\"==== Gradient check for skip-gram ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n",
    "        dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient),\n",
    "        dummy_vectors)\n",
    "    print(\"\\n==== Gradient check for CBOW      ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        cbow, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n",
    "        dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        cbow, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient),\n",
    "        dummy_vectors)\n",
    "\n",
    "    print(\"\\n=== Results ===\")\n",
    "    print((skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
    "        dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)))\n",
    "    print((skipgram(\"c\", 1, [\"a\", \"b\"],\n",
    "        dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset,\n",
    "        negSamplingCostAndGradient)))\n",
    "    print((cbow(\"a\", 2, [\"a\", \"b\", \"c\", \"a\"],\n",
    "        dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)))\n",
    "    print((cbow(\"a\", 2, [\"a\", \"b\", \"a\", \"c\"],\n",
    "        dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset,\n",
    "        negSamplingCostAndGradient)))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_word2vec()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmaxCostAndGradient(predicted, target, outputVectors, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.96735714, -0.02182641,  0.25247529],\n",
       "       [ 0.73663029, -0.48088687, -0.47552459],\n",
       "       [-0.27323645,  0.12538062,  0.95374082],\n",
       "       [-0.56713774, -0.27178229, -0.77748902],\n",
       "       [-0.59609459,  0.7795666 ,  0.19221644],\n",
       "       [-0.6831809 , -0.04200519,  0.72904007],\n",
       "       [ 0.18289107,  0.76098587, -0.62245591],\n",
       "       [-0.61517874,  0.5147624 , -0.59713884],\n",
       "       [-0.33867074, -0.80966534, -0.47931635],\n",
       "       [-0.52629529, -0.78190408,  0.33412466]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for skip-gram ====\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-502cf96322c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n\u001b[1;32m     17\u001b[0m     skipgram, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n\u001b[0;32m---> 18\u001b[0;31m     dummy_vectors)\n\u001b[0m\u001b[1;32m     19\u001b[0m gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n\u001b[1;32m     20\u001b[0m     skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient),\n",
      "\u001b[0;32m~/course/nlp/cs224n/assignment1/q2_gradcheck.py\u001b[0m in \u001b[0;36mgradcheck_naive\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mrndstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrndstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mfx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Evaluate function value at original point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m        \u001b[0;31m# Do not change this!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-502cf96322c2>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(vec)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==== Gradient check for skip-gram ====\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n\u001b[0;32m---> 17\u001b[0;31m     skipgram, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n\u001b[0m\u001b[1;32m     18\u001b[0m     dummy_vectors)\n\u001b[1;32m     19\u001b[0m gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
      "\u001b[0;32m<ipython-input-1-f3dce1812075>\u001b[0m in \u001b[0;36mword2vec_sgd_wrapper\u001b[0;34m(word2vecModel, tokens, wordVectors, dataset, C, word2vecCostAndGradient)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0minputVectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordVectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m     \u001b[0moutputVectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordVectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "dataset = type('dummy', (), {})()\n",
    "def dummySampleTokenIdx():\n",
    "    return random.randint(0, 4)\n",
    "\n",
    "def getRandomContext(C):\n",
    "    tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    return tokens[random.randint(0,4)], \\\n",
    "        [tokens[random.randint(0,4)] for i in range(2*C)]\n",
    "dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "dataset.getRandomContext = getRandomContext\n",
    "\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "print(\"==== Gradient check for skip-gram ====\")\n",
    "gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "    skipgram, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n",
    "    dummy_vectors)\n",
    "gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "    skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient),\n",
    "    dummy_vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
